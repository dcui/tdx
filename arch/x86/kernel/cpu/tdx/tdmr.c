// SPDX-License-Identifier: GPL-2.0
/*
 * Copyright(c) 2021 Intel Corporation.
 *
 * Intel Trusted Domain Extensions (TDX) memory initialization
 */

#define pr_fmt(fmt) "tdx: " fmt

#include <linux/types.h>
#include <linux/errno.h>
#include <linux/sizes.h>
#include <linux/slab.h>
#include <asm/e820/api.h>
#include <asm/pgtable.h>
#include "tdmr.h"

/*
 * Return whether the given e820 entry is RAM entry, and can be used
 * as TDX memory.  Note besides E820_TYPE_RAM, E820_TYPE_PRAM is also
 * considered as RAM since underneath it is RAM.  If user enables PRAM
 * via kernel command line on TDX enabled system, user is likely to want
 * to use PRAM as TD guest memory.
 *
 * Other types such as E820_TYPE_RESERVED_KERN or E820_TYPE_SOFT_RESERVED
 * may underneath also be RAM but they won't be used as TD guest memory
 * so they are not treated as RAM entries.
 */
static bool e820_entry_is_ram(struct e820_entry *entry)
{
	return (entry->type == E820_TYPE_RAM) ||
		(entry->type == E820_TYPE_PRAM);
}

/*
 * Get the memory region that is used by TDX for given e820 RAM entry
 * by skipping memory below 1MB.  In practice, memory below 1MB is not
 * included in CMRs generated by BIOS.  Don't treat those as TDX memory
 * so that they are not checked against CMRs and are not included into
 * TDMR's usable memory regions.  And skipping memory below 1MB is OK,
 * since they are reserved during kernel boot in setup_arch() and won't
 * be managed by page allocator anyway.
 *
 * Return true if the e820 entry is not completely skipped, in which case
 * the actual memory region after skipping memory below 1MB is returned
 * via @start and @end.  Otherwise caller should ignore this entry.
 */
static bool e820_entry_skip_lowmem(struct e820_entry *entry, u64 *start,
				   u64 *end)
{
	u64 _start = entry->addr;
	u64 _end = entry->addr + entry->size;

	if (_start < SZ_1M)
		_start = SZ_1M;

	*start = _start;
	*end = _end;

	return _start < _end;
}

/*
 * Helper to loop over all RAM entries which are used as TDX memory
 * in e820 table.  Both RAM and PRAM entries are treated as TDX memory.
 * Memory below 1MB is skipped.
 */
#define e820_for_each_ram_entry(_table, _i, _entry, _start, _end)	\
	for ((_i) = 0, (_entry) = &(_table)->entries[0];		\
			(_i) < (_table)->nr_entries;			\
			(_i)++, (_entry) = &(_table)->entries[(_i)])	\
		if (!e820_entry_is_ram((_entry)) ||			\
			!e820_entry_skip_lowmem((_entry),		\
				&(_start), &(_end))) { }		\
		else

/* Check whether first range is the subrange of the second */
static bool is_subrange(u64 r1_start, u64 r1_end, u64 r2_start, u64 r2_end)
{
	return (r1_start >= r2_start && r1_end <= r2_end) ? true : false;
}

/* Check whether address range is covered by any CMR or not. */
static bool range_covered_by_cmrs(struct cmr_info *cmr_array,
				  int cmr_num, u64 start, u64 end)
{
	int i;

	for (i = 0; i < cmr_num; i++) {
		struct cmr_info *cmr = &cmr_array[i];

		if (is_subrange(start, end, cmr->base, cmr->base + cmr->size))
			return true;
	}

	return false;
}

/*
 * Sanity check whether all e820 table RAM entries are fully
 * covered by CMRs.
 */
static int check_e820_against_cmrs(struct cmr_info *cmr_array, int cmr_num)
{
	struct e820_entry *entry;
	u64 start, end;
	int i;

	/*
	 * Loop over e820_table to find all RAM entries and check them
	 * against CMRs to see whether they are all fully covered by
	 * CMR.  Use e820_table instead of e820_table_firmware or
	 * e820_table_kexec to honor possible 'mem' and 'memmap' kernel
	 * command line.
	 */
	e820_for_each_ram_entry(e820_table, i, entry, start, end) {
		/*
		 * Check whether RAM entry is fully covered by any CMR.
		 * Only convertible memory can truly be used by TDX.
		 * If it is not, likely BIOS didn't generate CMRs
		 * correctly.  Give a message in this case.
		 */
		if (!range_covered_by_cmrs(cmr_array, cmr_num, start, end)) {
			pr_err("[0x%llx, 0x%llx) not fully convertible memory\n",
					start, end);
			return -EFAULT;
		}
	}

	return 0;
}

/* TDMR must be 1gb aligned */
#define TDMR_ALIGNMENT		BIT_ULL(30)
#define TDMR_PFN_ALIGNMENT	(TDMR_ALIGNMENT >> PAGE_SHIFT)

/* Align up and down one address to TDMR boundary */
#define TDMR_ALIGN_DOWN(_addr)	ALIGN_DOWN((_addr), TDMR_ALIGNMENT)
#define TDMR_ALIGN_UP(_addr)	ALIGN((_addr), TDMR_ALIGNMENT)

/* TDMR's start and end address */
#define TDMR_START(_tdmr)	((_tdmr)->base)
#define TDMR_END(_tdmr)		((_tdmr)->base + (_tdmr)->size)

/*
 * Create array of TDMR based on e820 entries.  The size of each TDMR
 * must be a whole of multiple of 1G size (and 1G aligned).  one TDMR
 * may include multiple e820 entries.
 */
static int create_tdmr_address_ranges(struct tdmr_info *tdmr_array,
				      int max_tdmr_num,
				      int max_rsvd_area_num,
				      int *tdmr_num)
{
	struct tdmr_info *tdmr, *prev_tdmr;
	struct e820_entry *entry;
	u64 start, end;
	int i, tdmr_idx;

	tdmr_idx = 0;
	tdmr = tdmr_array_entry(tdmr_array, 0, max_rsvd_area_num);
	prev_tdmr = NULL;
	/*
	 * Loop over all RAM entries that are used as TDX memory in e820
	 * table and create an array of TDMRs to cover them.  To keep it
	 * simple, always try to use one TDMR to cover one RAM entry.
	 *
	 * Since TDMR must be 1G aligned, when looping to a new RAM
	 * entry, it can be fully or partially covered by the TDMR which
	 * is created to cover previous RAM entry.  In the latter case,
	 * still create a new TDMR to cover the remaining part of the new
	 * RAM entry.
	 */
	e820_for_each_ram_entry(e820_table, i, entry, start, end) {
		/*
		 * @tdmr is the current valid TDMR that has been created
		 * to cover previous RAM entries, or it is the first TDMR
		 * whose address range has not been created when the loop
		 * starts with the first RAM entry.
		 *
		 * Check whether the RAM entry has already been covered
		 * by current TDMR.  If yes it's done.  For the first RAM
		 * entry, current TDMR's end is 0 and the check will fail.
		 */
		if (end <= TDMR_END(tdmr))
			continue;

		/*
		 * If current TDMR is a valid TDMR, by reaching here the
		 * new RAM entry is either totally above current TDMR, or
		 * it overlaps with current TDMR.  Move to the next TDMR
		 * in both cases to cover the new RAM entry.
		 */
		if (tdmr->size) {
			prev_tdmr = tdmr;
			tdmr_idx++;
			if (tdmr_idx >= max_tdmr_num)
				return -E2BIG;
			tdmr = tdmr_array_entry(tdmr_array, tdmr_idx,
				max_rsvd_area_num);
		}

		/*
		 * Set up the address range for the first TDMR to cover
		 * first RAM entry, or the new TDMR to cover the new RAM
		 * entry.  For the latter case, check whether the new RAM
		 * entry has been partially covered by previous TDMR, and
		 * set up new TDMR's base accordingly.
		 */
		tdmr->base = prev_tdmr &&
			(TDMR_END(prev_tdmr) > TDMR_ALIGN_DOWN(start)) ?
			TDMR_END(prev_tdmr) : TDMR_ALIGN_DOWN(start);
		tdmr->size = TDMR_ALIGN_UP(end) - tdmr->base;
	}

	/* @tdmr_idx is always the index of last valid TDMR. */
	*tdmr_num = tdmr_idx + 1;

	return 0;
}

/* Calculate PAMT size for one page size for given TDMR */
static unsigned long __tdmr_get_pamt_sz(struct tdmr_info *tdmr,
					enum tdx_page_sz pgsz,
					int pamt_entry_sz)
{
	unsigned long pamt_sz;

	pamt_sz = (tdmr->size >> ((9 * pgsz) + PAGE_SHIFT)) * pamt_entry_sz;
	/* PAMT size must be 4K aligned */
	pamt_sz = ALIGN(pamt_sz, PAGE_SIZE);

	return pamt_sz;
}

/*
 * Calculate PAMT size for one TDMR.  PAMTs for all supported page sizes
 * are calculated together as a whole to allow caller to allocate one
 * chunk of contiguous memory at once for all PAMTs.
 */
static unsigned long tdmr_get_pamt_sz(struct tdmr_info *tdmr,
				      int pamt_entry_sz_array[TDX_PG_MAX])
{
	enum tdx_page_sz pgsz;
	unsigned long pamt_sz;

	pamt_sz = 0;
	for (pgsz = TDX_PG_4K; pgsz < TDX_PG_MAX; pgsz++)
		pamt_sz += __tdmr_get_pamt_sz(tdmr, pgsz,
				pamt_entry_sz_array[pgsz]);

	return pamt_sz;
}

/* Get the NUMA node where PAMT will be allocated from for given TDMR */
static int tdmr_get_nid(struct tdmr_info *tdmr)
{
	struct e820_entry *entry;
	u64 start, end;
	int i;

	/* Find the first RAM entry covered by the TDMR */
	e820_for_each_ram_entry(e820_table, i, entry, start, end)
		if (end > TDMR_START(tdmr))
			break;

	/*
	 * One TDMR must cover at least one (or partial) RAM entry,
	 * otherwise it is kernel bug.  WARN_ON() in this case.
	 */
	if (WARN_ON(i == e820_table->nr_entries || start >= TDMR_END(tdmr)))
		return 0;

	return phys_to_target_node(start);
}

static int tdmr_setup_pamt(struct tdmr_info *tdmr,
			   int pamt_entry_sz_array[TDX_PG_MAX])
{
	unsigned long tdmr_pamt_base, pamt_base[TDX_PG_MAX];
	unsigned long pamt_sz[TDX_PG_MAX];
	unsigned long pamt_npages;
	struct page *pamt;
	enum tdx_page_sz pgsz;
	int nid;

	/*
	 * Allocate one chunk of physically contiguous memory for PAMTs
	 * for all supported page sizes together, although PAMTs for
	 * different page sizes are not required to be contiguous.  This
	 * helps to reduce TDMR's reserved areas occupied by PAMTs.
	 */
	nid = tdmr_get_nid(tdmr);
	pamt_npages = tdmr_get_pamt_sz(tdmr, pamt_entry_sz_array) >> PAGE_SHIFT;
	pamt = alloc_contig_pages(pamt_npages, GFP_KERNEL, nid,
			&node_online_map);
	if (!pamt)
		return -ENOMEM;

	/* Calculate PAMT base and size for all supported page sizes. */
	tdmr_pamt_base = page_to_pfn(pamt) << PAGE_SHIFT;
	for (pgsz = TDX_PG_4K; pgsz < TDX_PG_MAX; pgsz++) {
		unsigned long sz = __tdmr_get_pamt_sz(tdmr, pgsz,
				pamt_entry_sz_array[pgsz]);

		pamt_base[pgsz] = tdmr_pamt_base;
		pamt_sz[pgsz] = sz;

		tdmr_pamt_base += sz;
	}

	tdmr->pamt_4k_base = pamt_base[TDX_PG_4K];
	tdmr->pamt_4k_size = pamt_sz[TDX_PG_4K];
	tdmr->pamt_2m_base = pamt_base[TDX_PG_2M];
	tdmr->pamt_2m_size = pamt_sz[TDX_PG_2M];
	tdmr->pamt_1g_base = pamt_base[TDX_PG_1G];
	tdmr->pamt_1g_size = pamt_sz[TDX_PG_1G];

	return 0;
}

static void tdmr_free_pamt(struct tdmr_info *tdmr)
{
	unsigned long pamt_pfn, pamt_sz;

	pamt_pfn = tdmr->pamt_4k_base >> PAGE_SHIFT;
	pamt_sz = tdmr->pamt_4k_size + tdmr->pamt_2m_size + tdmr->pamt_1g_size;

	/* Do nothing if PAMT hasn't been allocated for this TDMR */
	if (!pamt_sz)
		return;

	WARN_ON(!pamt_pfn);
	free_contig_range(pamt_pfn, pamt_sz >> PAGE_SHIFT);
}

static void free_pamts_across_tdmrs(struct tdmr_info *tdmr_array, int tdmr_num,
				    int max_rsvd_area_num)
{
	int i;

	for (i = 0; i < tdmr_num; i++) {
		struct tdmr_info *tdmr = tdmr_array_entry(tdmr_array, i,
				max_rsvd_area_num);

		tdmr_free_pamt(tdmr);
	}
}

/* Allocate and set up PAMTs for all TDMRs */
static int setup_pamts_across_tdmrs(struct tdmr_info *tdmr_array, int tdmr_num,
				   int pamt_entry_sz_array[TDX_PG_MAX],
				   int max_rsvd_area_num)
{
	int i, ret;

	for (i = 0; i < tdmr_num; i++) {
		struct tdmr_info *tdmr = tdmr_array_entry(tdmr_array, i,
				max_rsvd_area_num);

		ret = tdmr_setup_pamt(tdmr, pamt_entry_sz_array);
		if (ret)
			goto err;
	}

	return 0;
err:
	free_pamts_across_tdmrs(tdmr_array, tdmr_num, max_rsvd_area_num);
	return -ENOMEM;
}

/**
 * construct_tdmrs - Construct TDMRs to cover all system RAM in e820
 *
 * @cmr_array:	Array of CMR_INFO
 * @cmr_num:	Number of CMR_INFO entries
 * @tdmr_array:	Array of constructed TDMRs
 * @desc:	TDX module descriptor for constructing TMDRs
 * @tdmr_num:	Actual number of TDMRs
 *
 * Construct TDMRs to cover all RAM entries in e820_table to convert
 * all system RAM to TDX memory.  The constructed TDMRs are stored in
 * @tdmr_array, with @tdmr_num reflects the actual TDMR number.
 *
 * Caller is responsible for allocating enough space for the array of
 * TDMRs.
 *
 * Return: 0 for success, or error.
 */
int construct_tdmrs(struct cmr_info *cmr_array, int cmr_num,
		    struct tdmr_info *tdmr_array,
		    struct tdx_module_descriptor *desc,
		    int *tdmr_num)
{
	int ret;

	ret = check_e820_against_cmrs(cmr_array, cmr_num);
	if (ret)
		goto err;

	ret = create_tdmr_address_ranges(tdmr_array, desc->max_tdmr_num,
			desc->max_rsvd_area_num, tdmr_num);
	if (ret)
		goto err;

	ret = setup_pamts_across_tdmrs(tdmr_array, *tdmr_num,
			desc->pamt_entry_size, desc->max_rsvd_area_num);
	if (ret)
		goto err;

	return 0;
err:
	return ret;
}
